{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdrFCT0clpZp",
        "outputId": "26c214b4-13fd-470c-9c71-ea1211dcf7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQEFvKT0lwDM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & DATABASE CONNECTION\n",
        "# ==========================================\n",
        "\n",
        "NEO4J_URI = \"XXXX\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"XXXX\"\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice, below code uses InterCorpu_v16ud_100k.csv and any_corp_AND_any_dict.tsv files. \n",
        "\n",
        "Please make sure to have these files in the same directory as the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvvSLOR9n3bn"
      },
      "outputs": [],
      "source": [
        "def load_and_prep_data():\n",
        "    print(\"Loading InterCorp data...\")\n",
        "    inter_df = pd.read_csv('InterCorp_v16ud_100k.csv', sep=';', on_bad_lines='skip')\n",
        "\n",
        "    # Clean columns and data\n",
        "    inter_df.columns = [c.strip().replace('\"', '') for c in inter_df.columns]\n",
        "    inter_df['freq'] = pd.to_numeric(inter_df['freq'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Get highest frequency entry for each word\n",
        "    inter_df = inter_df.sort_values('freq', ascending=False).drop_duplicates(subset=['word'])\n",
        "    stats_map = inter_df.set_index('word')[['upos', 'freq']].to_dict('index')\n",
        "\n",
        "    print(\"Loading Corpus words...\")\n",
        "    with open('any_corp_AND_any_dict.tsv', 'r', encoding='utf-8') as f:\n",
        "        corpus_words = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    # --- PRE-CALCULATION ---\n",
        "    # Sum the frequency of all words connected to a character\n",
        "    print(\"Calculating character frequencies...\")\n",
        "    char_freq_map = defaultdict(float)\n",
        "\n",
        "    for word in corpus_words:\n",
        "        # Get frequency of the word (default to 0 if not found)\n",
        "        w_freq = stats_map.get(word, {}).get('freq', 0)\n",
        "\n",
        "        # Add word's frequency to each of its UNIQUE characters\n",
        "        # Using set(word) means if a word is \"food\" (freq 100), 'o' gets +100 (not +200)\n",
        "        # This aligns with \"sum of the number of times the connected WORDS appear\"\n",
        "        for char in set(word):\n",
        "            char_freq_map[char] += w_freq\n",
        "\n",
        "    return corpus_words, stats_map, char_freq_map\n",
        "\n",
        "def create_chinese_nodes(tx, batch):\n",
        "    \"\"\"\n",
        "    Creates Word and Character nodes with consistent property naming.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    UNWIND $batch AS item\n",
        "\n",
        "    // 1. Create Word Node\n",
        "    MERGE (w:Word {id: item.word})\n",
        "    SET w.text = item.word,\n",
        "        w.pos = item.pos,\n",
        "        w.corpus_absolute_count = item.corpus_absolute_count,\n",
        "        w.corpus_log_count = item.corpus_log_count,\n",
        "        w.lang = item.lang\n",
        "\n",
        "    // 2. Process Characters\n",
        "    FOREACH (char_data IN item.chars |\n",
        "        MERGE (c:Character {id: char_data.char})\n",
        "        // Use the pre-calculated sums from the batch\n",
        "        SET c.text = char_data.char,\n",
        "            c.lang = item.lang,\n",
        "            c.corpus_absolute_count = char_data.corpus_absolute_count,\n",
        "            c.corpus_log_count = char_data.corpus_log_count\n",
        "\n",
        "        // Link Character -> Word\n",
        "        MERGE (c)-[r:COMPONENT]->(w)\n",
        "        SET r.order = char_data.order,\n",
        "            r.type = \"Compounding\"\n",
        "    )\n",
        "    \"\"\"\n",
        "    tx.run(query, batch=batch)\n",
        "\n",
        "def build_graph(driver, words, stats_map, char_freq_map):\n",
        "    batch_size = 2000\n",
        "    batch = []\n",
        "    total = len(words)\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Starting graph construction for {total} words...\")\n",
        "\n",
        "    with driver.session() as session:\n",
        "        for i, word in enumerate(words):\n",
        "\n",
        "            # 1. Word Stats\n",
        "            w_stats = stats_map.get(word, {'upos': 'UNKNOWN', 'freq': 0})\n",
        "            w_freq = float(w_stats['freq'])\n",
        "            w_log = math.log(w_freq + 1)\n",
        "\n",
        "            # 2. Character Data (Enriched)\n",
        "            chars_data = []\n",
        "            for idx, char in enumerate(word):\n",
        "                # Lookup the pre-summed frequency for this character\n",
        "                c_freq = char_freq_map.get(char, 0)\n",
        "                c_log = math.log(c_freq + 1)\n",
        "\n",
        "                chars_data.append({\n",
        "                    'char': char,\n",
        "                    'order': idx,\n",
        "                    # STRICT NAMING: Matches the node properties\n",
        "                    'corpus_absolute_count': c_freq,\n",
        "                    'corpus_log_count': c_log\n",
        "                })\n",
        "\n",
        "            # 3. Add to Batch\n",
        "            batch.append({\n",
        "                'word': word,\n",
        "                'pos': w_stats['upos'],\n",
        "                'lang': 'zh',\n",
        "                # STRICT NAMING: Matches the node properties\n",
        "                'corpus_absolute_count': w_freq,\n",
        "                'corpus_log_count': w_log,\n",
        "                'chars': chars_data\n",
        "            })\n",
        "\n",
        "            if len(batch) >= batch_size:\n",
        "                session.execute_write(create_chinese_nodes, batch)\n",
        "                batch = []\n",
        "                if (i + 1) % 10000 == 0:\n",
        "                    print(f\"Processed {i + 1}/{total} words... ({time.time() - start_time:.2f}s)\")\n",
        "\n",
        "        if batch:\n",
        "            session.execute_write(create_chinese_nodes, batch)\n",
        "\n",
        "    print(\"Graph construction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH3KuEJsq2y6",
        "outputId": "a027fb10-b074-4e0c-ceee-d0ae97fc1d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading InterCorp data...\n",
            "Loading Corpus words...\n",
            "Calculating character frequencies...\n",
            "Starting graph construction for 48881 words...\n",
            "Processed 10000/48881 words... (57.84s)\n",
            "Processed 20000/48881 words... (174.03s)\n",
            "Processed 30000/48881 words... (350.94s)\n",
            "Processed 40000/48881 words... (586.80s)\n",
            "Graph construction complete.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    words, stats, char_stats = load_and_prep_data()\n",
        "\n",
        "    build_graph(driver, words, stats, char_stats)\n",
        "\n",
        "    driver.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
