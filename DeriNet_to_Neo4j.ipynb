{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3KSMe8kqiwN"
      },
      "source": [
        "Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA47Q909jMdt",
        "outputId": "ab6a42ac-725c-45df-f8aa-abcc284acf77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
            "Collecting git+https://github.com/vidraj/derinet.git#subdirectory=tools/data-api/derinet2\n",
            "  Cloning https://github.com/vidraj/derinet.git to /tmp/pip-req-build-l2qgsarm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/vidraj/derinet.git /tmp/pip-req-build-l2qgsarm\n",
            "  Resolved https://github.com/vidraj/derinet.git to commit cc8af7386a80456ae2d731eac8fd34bff6ae49a0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installation complete and verified.\n"
          ]
        }
      ],
      "source": [
        "!pip install neo4j python-dotenv\n",
        "!pip install git+https://github.com/vidraj/derinet.git#subdirectory=tools/data-api/derinet2\n",
        "\n",
        "print(\"Installation complete and verified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oI_j2kaqiNN"
      },
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsnt6Jcwml8G",
        "outputId": "355d4d29-46f3-4979-a0e1-688b8bd0743a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration variables set.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from neo4j import GraphDatabase\n",
        "import derinet.lexicon as dlex\n",
        "import math\n",
        "# --- Configuration ---\n",
        "NEO4J_URI = \"XXXX\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"XXXX\"\n",
        "\n",
        "DERINET_DATA_PATH = \"derinet-2-3.tsv\"\n",
        "BATCH_SIZE = 5000\n",
        "\n",
        "os.environ['NEO4J_URI'] = NEO4J_URI\n",
        "os.environ['NEO4J_USERNAME'] = NEO4J_USERNAME\n",
        "os.environ['NEO4J_PASSWORD'] = NEO4J_PASSWORD\n",
        "\n",
        "print(\"Configuration variables set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vOwB5-zqyI8"
      },
      "source": [
        "Downloading DeriNet 2.3 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-jz69XFnYwM",
        "outputId": "d8599ace-e200-4e6f-d5a2-4fa71c3c56f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading DeriNet 2.3 data package via curl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  418M    0  418M    0     0  27.3M      0 --:--:--  0:00:15 --:--:-- 31.0M\n",
            "✅ Data package downloaded as derinet-2-3-all.zip.\n",
            "Archive:  derinet-2-3-all.zip\n",
            "  inflating: ./derinet-2-3.tsv       \n",
            "✅ Data extracted. The target file path is set to: derinet-2-3.tsv\n"
          ]
        }
      ],
      "source": [
        "DERINET_ZIP_URL = \"https://lindat.mff.cuni.cz/repository/server/api/core/items/62540779-b206-4cf7-ac33-399ce68e35e6/allzip?handleId=11234/1-5846\"\n",
        "LOCAL_ZIP_FILENAME = \"derinet-2-3-all.zip\"\n",
        "DERINET_DATA_PATH = \"derinet-2-3.tsv\"\n",
        "\n",
        "print(f\"Downloading DeriNet 2.3 data package via curl...\")\n",
        "!curl -o $LOCAL_ZIP_FILENAME $DERINET_ZIP_URL\n",
        "\n",
        "print(f\"✅ Data package downloaded as {LOCAL_ZIP_FILENAME}.\")\n",
        "!unzip -o $LOCAL_ZIP_FILENAME -d .\n",
        "\n",
        "print(f\"✅ Data extracted. The target file path is set to: {DERINET_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFNOUaZQGBvz"
      },
      "source": [
        "Queries and Node Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zWivJzZo5l-"
      },
      "outputs": [],
      "source": [
        "# Create unique constraint on the Word ID\n",
        "CONSTRAINT_QUERY = \"CREATE CONSTRAINT word_id_unique IF NOT EXISTS FOR (w:Word) REQUIRE w.id IS UNIQUE;\"\n",
        "\n",
        "# MERGE Nodes with all properties\n",
        "# Note: Complex dicts/lists are stored as JSON strings because Neo4j properties must be primitives.\n",
        "NODE_MERGE_QUERY = \"\"\"\n",
        "UNWIND $words AS word_data\n",
        "MERGE (w:Word {id: word_data.id})\n",
        "ON CREATE SET\n",
        "    w.lemma = word_data.lemma,\n",
        "    w.pos = word_data.pos,\n",
        "    w.language = word_data.lang,\n",
        "    w.is_root = word_data.is_root,\n",
        "    w.features = word_data.features,\n",
        "    w.morphology = word_data.morphology,\n",
        "    w.misc = word_data.misc,\n",
        "    w.corpus_stats = word_data.corpus_stats,\n",
        "    w.corpus_absolute_count = word_data.corpus_absolute_count\n",
        "\"\"\"\n",
        "\n",
        "# MERGE Relationships\n",
        "REL_MERGE_QUERY = \"\"\"\n",
        "UNWIND $relations AS rel_data\n",
        "MATCH (parent:Word {id: rel_data.parent_id})\n",
        "MATCH (child:Word {id: rel_data.child_id})\n",
        "MERGE (parent)-[r:COMPONENT {type: rel_data.type}]->(child)\n",
        "\"\"\"\n",
        "\n",
        "def create_index(tx):\n",
        "    tx.run(CONSTRAINT_QUERY)\n",
        "\n",
        "def batch_insert(driver, query, data_list, name=\"Items\"):\n",
        "    print(f\"\\nStarting {name} insertion...\")\n",
        "    total_count = len(data_list)\n",
        "\n",
        "    if total_count == 0:\n",
        "        print(f\"No {name} to insert.\")\n",
        "        return\n",
        "\n",
        "    for i in range(0, total_count, BATCH_SIZE):\n",
        "        batch = data_list[i:i + BATCH_SIZE]\n",
        "        try:\n",
        "            with driver.session() as session:\n",
        "                session.run(query, {name.lower(): batch})\n",
        "            print(f\"-> Batch {i//BATCH_SIZE + 1} of {total_count//BATCH_SIZE + 1} processed ({len(batch)} {name}).\")\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error in {name} batch starting at index {i}. Stopping: {e}\")\n",
        "            raise\n",
        "\n",
        "def prepare_data(lexicon):\n",
        "    word_nodes = []\n",
        "    word_relations = []\n",
        "\n",
        "    print(\"Iterating over lexicon to prepare data...\")\n",
        "\n",
        "    for lexeme in lexicon.iter_lexemes():\n",
        "\n",
        "        primary_parent_id = lexeme.parent.lemid if lexeme.parent else None\n",
        "\n",
        "        # Parse JSON stats\n",
        "        corpus_stats_str = \"{}\"\n",
        "        absolute_count = 0\n",
        "        if hasattr(lexeme, 'extra_data') and lexeme.extra_data:\n",
        "             if 'corpus_stats' in lexeme.extra_data:\n",
        "                stats = lexeme.extra_data['corpus_stats']\n",
        "                corpus_stats_str = json.dumps(stats)\n",
        "                absolute_count = stats.get('absolute_count', 0)\n",
        "\n",
        "        morph_json = json.dumps(lexeme.segmentation) if hasattr(lexeme, 'segmentation') else \"[]\"\n",
        "\n",
        "        word_nodes.append({\n",
        "            \"id\": lexeme.lemid,\n",
        "            \"lemma\": lexeme.lemma,\n",
        "            \"pos\": str(lexeme.pos),\n",
        "            \"lang\": getattr(lexeme, 'lang', 'cs'),\n",
        "            \"is_root\": primary_parent_id is None,\n",
        "            \"features\": json.dumps(lexeme.feats),\n",
        "            \"misc\": json.dumps(lexeme.misc),\n",
        "            \"morphology\": morph_json,\n",
        "            \"corpus_stats\": corpus_stats_str,\n",
        "            \"corpus_absolute_count\": absolute_count,\n",
        "            \"corpus_log_count\": math.log(absolute_count + 1)\n",
        "        })\n",
        "\n",
        "        if hasattr(lexeme, 'parent_relations'):\n",
        "            for rel in lexeme.parent_relations:\n",
        "                rel_type = getattr(rel, 'type', 'Derivation')\n",
        "\n",
        "                # A single relation (like Compounding) can have multiple sources.\n",
        "                # We iterate over all sources for this specific relation.\n",
        "                for source_lexeme in rel.sources:\n",
        "                    word_relations.append({\n",
        "                        \"child_id\": lexeme.lemid,\n",
        "                        \"parent_id\": source_lexeme.lemid,\n",
        "                        \"type\": rel_type\n",
        "                    })\n",
        "\n",
        "    return word_nodes, word_relations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZXqkCM-9gRn"
      },
      "source": [
        "Batch Uploading/Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ofLZdv8ojK7",
        "outputId": "ad217658-6dae-433e-fbfd-93298bd2ff75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DeriNet data from derinet-2-3.tsv...\n",
            "✅ DeriNet file loaded successfully.\n",
            "Iterating over lexicon to prepare data...\n",
            "✅ Data prepared. Nodes: 1042751, Relationships: 857538\n",
            "Filtering word nodes by corpus_absolute_count. Total nodes: 1042751\n",
            "✅ Filtered nodes to top 75,000 based on frequency. Remaining nodes: 75000\n",
            "✅ Filtered relationships. Original: 857538, Remaining: 35272\n",
            "\n",
            "✅ Connection to Neo4j established.\n",
            "✅ Unique constraint on :Word(id) created.\n",
            "\n",
            "Starting Words insertion...\n",
            "-> Batch 1 of 16 processed (5000 Words).\n",
            "-> Batch 2 of 16 processed (5000 Words).\n",
            "-> Batch 3 of 16 processed (5000 Words).\n",
            "-> Batch 4 of 16 processed (5000 Words).\n",
            "-> Batch 5 of 16 processed (5000 Words).\n",
            "-> Batch 6 of 16 processed (5000 Words).\n",
            "-> Batch 7 of 16 processed (5000 Words).\n",
            "-> Batch 8 of 16 processed (5000 Words).\n",
            "-> Batch 9 of 16 processed (5000 Words).\n",
            "-> Batch 10 of 16 processed (5000 Words).\n",
            "-> Batch 11 of 16 processed (5000 Words).\n",
            "-> Batch 12 of 16 processed (5000 Words).\n",
            "-> Batch 13 of 16 processed (5000 Words).\n",
            "-> Batch 14 of 16 processed (5000 Words).\n",
            "-> Batch 15 of 16 processed (5000 Words).\n",
            "\n",
            "Starting Relations insertion...\n",
            "-> Batch 1 of 8 processed (5000 Relations).\n",
            "-> Batch 2 of 8 processed (5000 Relations).\n",
            "-> Batch 3 of 8 processed (5000 Relations).\n",
            "-> Batch 4 of 8 processed (5000 Relations).\n",
            "-> Batch 5 of 8 processed (5000 Relations).\n",
            "-> Batch 6 of 8 processed (5000 Relations).\n",
            "-> Batch 7 of 8 processed (5000 Relations).\n",
            "-> Batch 8 of 8 processed (272 Relations).\n",
            "\n",
            "✨ Data import complete.\n"
          ]
        }
      ],
      "source": [
        "def convert_derinet_to_neo4j():\n",
        "    # Variables loaded from environment or defined earlier in Block 1\n",
        "    URI = os.getenv(\"NEO4J_URI\")\n",
        "    AUTH = (os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
        "\n",
        "    print(f\"Loading DeriNet data from {DERINET_DATA_PATH}...\")\n",
        "    lexicon = None\n",
        "    try:\n",
        "        lexicon = dlex.Lexicon()\n",
        "        lexicon.load(data_source=DERINET_DATA_PATH, fmt=dlex.Format.DERINET_V2)\n",
        "        print(\"✅ DeriNet file loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load DeriNet data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Call the corrected prepare_data function from Block 3\n",
        "    word_nodes, word_relations = prepare_data(lexicon)\n",
        "\n",
        "    num_words = len(word_nodes)\n",
        "    num_rels = len(word_relations)\n",
        "    if num_words == 0:\n",
        "        print(\"❌ Error: Lexicon yielded 0 nodes.\")\n",
        "        return\n",
        "\n",
        "    print(f\"✅ Data prepared. Nodes: {num_words}, Relationships: {num_rels}\")\n",
        "\n",
        "    # --- Filter Word Nodes by Frequency ---\n",
        "    print(f\"Filtering word nodes by corpus_absolute_count. Total nodes: {len(word_nodes)}\")\n",
        "    word_nodes.sort(key=lambda x: x['corpus_absolute_count'], reverse=True)\n",
        "    word_nodes = word_nodes[:75000]\n",
        "    top_75k_word_ids = {node['id'] for node in word_nodes}\n",
        "\n",
        "    # Filter word_relations to only include relations where both parent and child are in the top 75k words\n",
        "    original_relations_count = len(word_relations)\n",
        "    word_relations = [rel for rel in word_relations if rel['parent_id'] in top_75k_word_ids and rel['child_id'] in top_75k_word_ids]\n",
        "\n",
        "    print(f\"✅ Filtered nodes to top 75,000 based on frequency. Remaining nodes: {len(word_nodes)}\")\n",
        "    print(f\"✅ Filtered relationships. Original: {original_relations_count}, Remaining: {len(word_relations)}\")\n",
        "\n",
        "    # Connect to Neo4j and Import\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "        # Verify connection before starting large transactions\n",
        "        driver.verify_connectivity()\n",
        "        print(\"\\n✅ Connection to Neo4j established.\")\n",
        "\n",
        "        # Create constraint/index\n",
        "        with driver.session() as session:\n",
        "            session.execute_write(create_index)\n",
        "        print(\"✅ Unique constraint on :Word(id) created.\")\n",
        "\n",
        "        # Insert Nodes\n",
        "        batch_insert(driver, NODE_MERGE_QUERY, word_nodes, name=\"Words\")\n",
        "\n",
        "        # Insert Relationships\n",
        "        batch_insert(driver, REL_MERGE_QUERY, word_relations, name=\"Relations\")\n",
        "\n",
        "        print(\"\\n✨ Data import complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ A database error occurred: {e}\")\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_derinet_to_neo4j()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6Sw9z3tuKqE",
        "outputId": "c966c2a1-e151-4222-f8b3-57b10ccbe968"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nSome useful neo4j commands:\\n\\nThis finds nodes which have more than 2 unique relation types\\nMATCH (w:Word)-[r:DERIVED_FROM]->(child)\\nWITH w, count(DISTINCT r.type) AS unique_types\\nWHERE unique_types > 2\\nRETURN w.lemma, unique_types\\n\\nThis finds nodes with the corresponding lemma\\nMATCH (n:Word {lemma: 'kilogram'})\\nRETURN n\\n\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Some useful neo4j commands:\n",
        "\n",
        "This finds nodes which have more than 2 unique relation types\n",
        "MATCH (w:Word)-[r:DERIVED_FROM]->(child)\n",
        "WITH w, count(DISTINCT r.type) AS unique_types\n",
        "WHERE unique_types > 2\n",
        "RETURN w.lemma, unique_types\n",
        "\n",
        "This finds nodes with the corresponding lemma\n",
        "MATCH (n:Word {lemma: 'kilogram'})\n",
        "RETURN n\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
